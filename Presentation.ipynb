{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering on strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import gensim.corpora as corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",header=None,low_memory=False)\n",
    "data_test = pd.read_csv(\"test.csv\",header=None,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data[1][1:]\n",
    "labels = data[2][1:]\n",
    "sentences_test = []\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some analysis on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Brief description of the data}$\n",
    " - Number of sentences (documents) in the whole corpus $\\approx$ 1.3 milion\n",
    " - Number of words $\\approx$ 16 milion\n",
    " - Number of unique words = 97500\n",
    "\n",
    "$\\textbf{Task description}$\n",
    "\n",
    "Given a question (a string) of variable length, predict if that question is sincere or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer().fit(sentences)\n",
    "bag_of_words = vec.transform(sentences)\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Most frequent 10 words in the dataset.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'the' appears 665950 times.\n",
      "Word 'what' appears 471294 times.\n",
      "Word 'is' appears 443182 times.\n",
      "Word 'to' appears 408009 times.\n",
      "Word 'in' appears 378153 times.\n",
      "Word 'of' appears 333515 times.\n",
      "Word 'how' appears 290405 times.\n",
      "Word 'and' appears 257922 times.\n",
      "Word 'do' appears 253252 times.\n",
      "Word 'are' appears 243038 times.\n"
     ]
    }
   ],
   "source": [
    "for word, freq in words_freq[:10]:\n",
    "    print(\"Word \" + \"'\\033[1m\"+str(word) +\"\\033[0m'\" + \" appears \" + str(freq) + \" times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset are 15999712 of which 97500 are distinct.\n"
     ]
    }
   ],
   "source": [
    "frequencies = np.array(words_freq)\n",
    "frequencies = [int(x) for x in frequencies[:,1]]\n",
    "distinct_words = int(len(vec.vocabulary_.items())/2)\n",
    "print(\"Total number of words in the dataset are \" + \n",
    "      \"\\033[1m\"+str(np.sum(frequencies)) +\"\\033[0m\"+ \" of which \" + \n",
    "      \"\\033[1m\"+str(distinct_words)+\"\\033[0m\" + \" are distinct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{10 of the least frequent words in the dataset (found only once in the corpus).}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'uniersity' appears one time.\n",
      "Word 'wheath' appears one time.\n",
      "Word 'subjested' appears one time.\n",
      "Word 'notafcation' appears one time.\n",
      "Word 'faulds' appears one time.\n",
      "Word 'abody' appears one time.\n",
      "Word '15260' appears one time.\n",
      "Word 'localbitcoins' appears one time.\n",
      "Word 'issil' appears one time.\n",
      "Word 'c720' appears one time.\n"
     ]
    }
   ],
   "source": [
    "for word, freq in words_freq[-97500:-97490]:\n",
    "    print(\"Word \" + \"\\033[1m'\"+str(word) +\"'\\033[0m\" + \" appears one time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1960s 1)  (as 1)  (did 1)  (how 1)  (in 1)  (nation 1)  (nationalists 1)  (province 1)  (quebec 1)  (see 1)  (the 1)  (their 1)  \n",
      "How did Quebec nationalists see their province as a nation in the 1960s?\n",
      "\n",
      "(are 1)  (babies 3)  (dark 1)  (light 1)  (more 1)  (or 1)  (parents 1)  (skin 2)  (sweeter 1)  (their 1)  (to 1)  (which 1)  \n",
      "Which babies are more sweeter to their parents? Dark skin babies or light skin babies?\n"
     ]
    }
   ],
   "source": [
    "sincere_question_count = (bag_of_words[0].todense())\n",
    "sincere_question_words = np.argwhere(sincere_question_count>=1)\n",
    "for word in sincere_question_words:\n",
    "    print(\"(\"+vec.get_feature_names()[word[1]] + \" \" \n",
    "          + str(sincere_question_count[0,word[1]])+\")\", end=\"  \")\n",
    "print(\"\\n\" + sentences[1]+\"\\n\")\n",
    "lbls = labels.values\n",
    "insincere_example = np.argwhere(lbls == '1')[1,0]\n",
    "insincere_question_count = (bag_of_words[insincere_example].todense())\n",
    "insincere_question_words = np.argwhere(insincere_question_count>=1)\n",
    "for word in insincere_question_words:\n",
    "    print(\"(\"+vec.get_feature_names()[word[1]] + \" \" \n",
    "          + str(insincere_question_count[0,word[1]]) + \")\", end=\"  \")\n",
    "print(\"\\n\" + sentences[insincere_example+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[2][1:]\n",
    "Y = Y.values\n",
    "vectorizer = CountVectorizer(min_df=5)\n",
    "sentences = data[1][1:]\n",
    "X = vectorizer.fit_transform(list(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Additional useful parameters for CountVectorizer:}$\n",
    " - lowercase = True. \n",
    " - analyzer = ‘char_wb’: change words into character n-grams ( \"apple\" word has \\[\"app\", \"ppl\", \"ple\"\\] 3-grams ).\n",
    " - max_df/min_df = maximum/minimum character frequency in the document (sentence in our case) for the word (or gram) to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\n",
    "LR_model = LogisticRegressionCV(cv=5,random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "preds = LR_model.predict(X_test)\n",
    "preds = [int(x) for x in preds]\n",
    "y_test = [int(x) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for bags of words: 0.5400183290056515\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score on LR for bags of words: \" + str(f1_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency- inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Instead of using the frequency of a word (that is, the number of times it appears in a sentence) in a given document (in our case, a sentence), we are going to furtherly tune the parameters so that extremly frequent words throughout the whole set of documents - like the words \"the\", \"what\", \"is\" and so on, shown above to have very high frequencies - will be taken less into acount, as they do not tell much about the nature of a given question.\n",
    "### $$tfidf(t,d,D) = tf(t,d)\\cdot idf(t,D)$$\n",
    "The above function multiplies the frequency of a word $t$ in the current document (sentence in our case) by the inverse frequency of that word in all the documents. These two functions, $tf$ and $idf$ are defined as:\n",
    "### $$idf(t,D)=log\\frac{N}{|\\{d\\in D : t\\in d\\}|}$$ \n",
    "with\n",
    " - $N$: number of documents in the corpus.\n",
    " - $|\\{d \\in D : t \\in d\\}|$: number of documents in the corpus in which t appears.\n",
    " and for the term-frequency fucntion\n",
    "### $$tf(t,d) = 0.5 + 0.5 \\cdot \\frac{f_{t,d}}{max\\{f_{t',d}:t'\\in d\\}}$$\n",
    "Which is an augmented frequency defined as the raw frequency of a word in the given document, divided by the maximum frequency of any element in that document (although mostly useless in our case, since this adjustment is done to prevent bias towards longer documents, and all of our documents are roughly the same size, them being only one question each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(min_df = 5)\n",
    "X = tfidf_vec.fit_transform(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\n",
    "LR_model = LogisticRegressionCV(cv=5,random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "preds = LR_model.predict(X_test)\n",
    "preds = [int(x) for x in preds]\n",
    "y_test = [int(x) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for bags of words: 0.5530106434991914\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score on LR for bags of words: \" + str(f1_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram and continuous bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the Word2Vec methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Vocabulary}$\n",
    " - decide what vocabulary size (select words from the document based on their number of appearances)\n",
    " - for each word in the vocabulary, assign a unique index and with those indeces, create one_hot vectors for them $x=[0,0,...,1, ... 0]$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/window.png\"  style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Skip gram}$\n",
    " - Try predicting a context (words frequently found nearby), given a single word input.\n",
    " - Define a window size $n$ (for example, 5, as in the image above), and pick the words that are $n$ close to the word picked in a sentence (left and right)\n",
    " - Train a fully connected neural network with one hidden layer, predicting words found close to it. (For sentence \"a b c d e\", pick 'c' as the input and \"a b d e\" as the output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/SKIP_GRAM.png\"  style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Continuous bags of words (CBOW)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - similar to skip-gram, but \"flip\" the training inputs and outputs, i.e. predict a target output given context (words found close to it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/CBOW.png\"  style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gensim implementation:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Edit the sentences}$ (get rid of the punctuation, lower-case the strings and split them into words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "sentences = data[1][1:]\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "embedding_sentences = []\n",
    "for sentence in sentences:\n",
    "    a = sentence.translate(translator).lower().split()\n",
    "    embedding_sentences.append(a)\n",
    "    all_words.extend(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Parameters used in our model:}$\n",
    " - size = 300: the size of the embedding of each word in the vocabulary\n",
    " - window = 5: window of context used, as described in the CBOW/Skip-gram models above\n",
    " - min_count = 5: minimum frequency for a word to be taken into account in the dictionary\n",
    " - workers = 10: paralel working specification when working with multi-core machines\n",
    " - sg = 1: not used here, but will be used for the skip-gram model (it just tells the model to train using skip-gram method)\n",
    "\n",
    "$\\textbf{Train a CBOW model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120006542, 167042420)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        embedding_sentences,\n",
    "        size=300,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        seed=1,\n",
    "        workers=10)\n",
    "model.train(embedding_sentences, total_examples=len(embedding_sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Save the model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CBOW_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Reload it}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_gbow_model  = gensim.models.Word2Vec.load(\"CBOW_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Get similar words for some chosen words}$ \n",
    "\n",
    "Note that these models, the words need to be found in the dictionary - this problem will be dealt with in the FastText method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.7663631439208984), ('man', 0.7301400303840637), ('women', 0.6900008916854858), ('lady', 0.6801614761352539), ('person', 0.6645864248275757)]\n",
      "[('emperor', 0.5211741924285889), ('kings', 0.5113060474395752), ('queen', 0.4816433787345886), ('solomon', 0.4810170531272888), ('monarch', 0.4801141023635864)]\n",
      "[('puppy', 0.7459352016448975), ('kitten', 0.7062520980834961), ('hamster', 0.6700388789176941), ('cat', 0.6575609445571899), ('dogs', 0.6490512490272522)]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_gbow_model.wv.similar_by_word(\"woman\",5))\n",
    "print(loaded_gbow_model.wv.similar_by_word(\"king\",5))\n",
    "print(loaded_gbow_model.wv.similar_by_word(\"dog\",5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Train a Skip-gram model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120005737, 167042420)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        embedding_sentences,\n",
    "        size=300,\n",
    "        window=5,\n",
    "        seed=1,\n",
    "        sg=1,\n",
    "        min_count=5,\n",
    "        workers=10)\n",
    "model.train(embedding_sentences, total_examples=len(embedding_sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Save the model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"SG_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Reload it}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sg_model = gensim.models.Word2Vec.load(\"SG_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Get similar words for some chosen words}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.7393679618835449), ('women', 0.6713191270828247), ('girl', 0.6492231488227844), ('35yearold', 0.5798226594924927), ('milf', 0.5780065059661865)]\n",
      "[('tut', 0.5243228673934937), ('queen', 0.5077412128448486), ('kings', 0.5044434666633606), ('vikramaditya', 0.480135440826416), ('hrh', 0.4775933623313904)]\n",
      "[('puppy', 0.6731647253036499), ('dogs', 0.6642792224884033), ('cats', 0.5948817729949951), ('kitten', 0.5760080218315125), ('husky', 0.563368022441864)]\n"
     ]
    }
   ],
   "source": [
    "print(load_sg_model.wv.similar_by_word(\"woman\",5))\n",
    "print(load_sg_model.wv.similar_by_word(\"king\",5))\n",
    "print(load_sg_model.wv.similar_by_word(\"dog\",5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "Extension of the Word2Vec models. The idea behind FastText is to use n-grams instead of words. An n-gram is a group of letters taken from the actual word (e.g., the 3-gram for \"apple\" will be \"app\", \"ppl\", \"ple\"), and the actual final embedding for the word will be the summ of all it's n-grams.\n",
    "\n",
    "What is great about this method is that we can extract a context (meaning) vector even for words that do not exist at all in the dictionary we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted = FastText(\n",
    "    embedding_sentences, \n",
    "    size=300, \n",
    "    window=5, \n",
    "    min_count=5, \n",
    "    workers=10,\n",
    "    sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted.save(\"FT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ft_model = gensim.models.FastText.load(\"FT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girlwoman', 0.887147068977356), ('womanman', 0.88193678855896), ('manwoman', 0.8607190847396851), ('womanly', 0.8196729421615601), ('woman’s', 0.7845233678817749)]\n",
      "[('kingpin', 0.8398256301879883), ('king’s', 0.8095637559890747), ('kingqueen', 0.7933975458145142), ('kingsley', 0.7807847857475281), ('kingsguard', 0.7293356657028198)]\n",
      "[('dogcat', 0.7769787311553955), ('dogs', 0.76622474193573), ('dog’s', 0.7232863903045654), ('dogg', 0.7129536271095276), ('puppy', 0.7121647596359253)]\n"
     ]
    }
   ],
   "source": [
    "print(load_ft_model.wv.similar_by_word(\"woman\",5))\n",
    "print(load_ft_model.wv.similar_by_word(\"king\",5))\n",
    "print(load_ft_model.wv.similar_by_word(\"dog\",5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that \"Gastroenteritis\" is not present in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word is not in the document\n"
     ]
    }
   ],
   "source": [
    "word_fq_array = np.array(words_freq)\n",
    "unique_words = dict.fromkeys(word_fq_array[:,0],1)\n",
    "if \"Gastroenteritis\" not in unique_words:\n",
    "    print(\"The word is not in the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Get top 10 similar words to a word that is not found in the vocabulary}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similar meaning words according to the model. Notice how the fast text model will still produce decent similar words for the word \"Gastroenteritis\", even if we didn't have that word in the training corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gastroenterologist', 0.6894631385803223), ('gastroenterology', 0.6843836307525635), ('dnb', 0.6726697087287903), ('orthopedics', 0.6438664197921753), ('orthopedic', 0.6283782720565796), ('pediatrics', 0.6272794008255005), ('dmc', 0.6225731372833252), ('ludhiana', 0.6222456693649292), ('pgi', 0.6205636262893677), ('visakhapatnam', 0.6173926591873169)]\n"
     ]
    }
   ],
   "source": [
    "print(load_ft_model.wv.most_similar(\"Gastroenteritis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Train a LR model with cross-validation using a fraction of the data*.}$\n",
    "\n",
    "*Since both converting the words into word-vectors and the actual training on the data, after splitting, the training was done in a fraction of the data, containing: all the insincere questions and 4 times that number of sincere ones. Training was done on the average vector representation in a given sentence, out of all the word-embeddings in one sentence at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_tf_model():\n",
    "    i = 0\n",
    "    Y = data[2][1:]\n",
    "    Y = Y.values\n",
    "    X = embedding_sentences\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05)\n",
    "    X_train = np.array(X_train)\n",
    "    insincere = X_train[y_train == '1'][:]\n",
    "    sincere = X_train [y_train == '0'][:]\n",
    "    a = []\n",
    "    y = []\n",
    "    for sent in insincere:\n",
    "        word_vectors = [] \n",
    "        for single_word in sent:\n",
    "            if single_word in load_ft_model.wv.vocab: \n",
    "                word_vectors.append(load_ft_model.wv.word_vec(single_word))\n",
    "        word_vectors = np.array(word_vectors)\n",
    "        if len(word_vectors) != 0:                \n",
    "            if word_vectors.shape[1] == 300:\n",
    "                a.append(np.mean(word_vectors,axis=0))\n",
    "                y.append(1)\n",
    "    for sent in sincere[:64674*4]:\n",
    "        word_vectors = []  \n",
    "        for single_word in sent:\n",
    "            if single_word in load_ft_model.wv.vocab: \n",
    "                word_vectors.append(load_ft_model.wv.word_vec(single_word))\n",
    "        word_vectors = np.array(word_vectors)\n",
    "        if len(word_vectors) != 0:\n",
    "            if word_vectors.shape[1] == 300:\n",
    "\n",
    "                a.append(np.mean(word_vectors,axis=0))\n",
    "                y.append(0)\n",
    "    a = np.array(a)\n",
    "    y = np.array(y)\n",
    "    LR_model = LogisticRegressionCV(cv=5,random_state=0, solver='lbfgs').fit(a, y)\n",
    "    x_test = []\n",
    "    indexes = []\n",
    "    for index, sent in enumerate(X_test):\n",
    "        word_vectors = []     \n",
    "        for single_word in sent:\n",
    "            if single_word in load_ft_model.wv.vocab: \n",
    "                word_vectors.append(load_ft_model.wv.word_vec(single_word))\n",
    "        word_vectors = np.array(word_vectors)\n",
    "        if len(word_vectors) != 0:   \n",
    "            if word_vectors.shape[1] == 300:\n",
    "                x_test.append(np.mean(word_vectors,axis=0))\n",
    "            else:\n",
    "                indexes.append(index)\n",
    "        else:\n",
    "            indexes.append(indexes)\n",
    "    \n",
    "    preds = LR_model.predict(x_test)\n",
    "    preds = [int(x) for x in preds]\n",
    "    y_test = [int(x) for x in y_test]\n",
    "    y_test = np.array(y_test)\n",
    "    if len(indexes) != 0:\n",
    "        y_test = np.delete(y_test,indexes)\n",
    "    print(\"F1 Score on LR for Fast text sentence embeddings: \" + str(f1_score(y_test,preds)))\n",
    "    return LR_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for Fast text sentence embeddings: 0.5380184331797235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=0,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lr_tf_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Vectors for Word Representation (Glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - matrix $X$ is the matrix of word co-occurances, in which $X_{ij}$ represents the number of times word $i$ occurs in the context of word $j$. \n",
    " - $X_{i} = \\sum_{k} X_{ik}$ be the total number of occurances in any context k for work i.\n",
    " - Finally, $P_{ij}=P(j|i)=X_{ij}/X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Probability and Ratio |$k = solid$|$k = gas$|$k = water$|$k = fashion$|\n",
    "|-----------------------|-----------|---------|-----------|-------------|\n",
    "|  $P(k|ice)$           |$1.9\\times 10^{-4}$|$6.6 \\times 10^{-5}$|$3.0\\times 10^{-3}$|$1.7\\times 10^{-5}$|\n",
    "|  $P(k|steam)$         |$2.2\\times 10^{-5}$|$7.8\\times 10^{-4}$|$2.2\\times 10^{-3}$|$1.8\\times 10^{-5}$|\n",
    "|  $P(k|ice)/P(k|steam)$|$$8.9$$            |$$8.5\\times 10^{-2}$$|$$1.36           $$|$$0.96$$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the probability ratio for various words $k$ illustrates pretty well the semantic similarity of two words $i$ and $j$.\n",
    " - k related to ice and not to steam: high ratio\n",
    " - k related to steam but not to ice: small ratio\n",
    " - k related to both: ratio close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization function proposed by Glove tries to minimize the below J function. Note that $V$ here represents the number of words in the whole vocabulary and the $f$ function will weight down the very frequent words (similar to the problem of link words described in the Word2Vec approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$J=\\sum_{i,j=1}^{V} f(X_{ij})(w_i^T\\tilde{w_j}+b_i+\\tilde{b_j}-logX_{ij})^2$$\n",
    "\n",
    "### $$f(X_{ij})= \\begin{cases} \n",
    "      (x/x_{max})^\\alpha & if x < x_{max} \\\\\n",
    "      1 & otherwise \\\\\n",
    "   \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Load a pretrained spacy glove model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model =  spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.EntityRecognizer at 0x2995b3fc518>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sentences.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Train a Glove embedded model on the same amounts of data as in FastText embedding}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[2][1:]\n",
    "Y = Y.values\n",
    "sentences = data[1][1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05)\n",
    "insincere = X_train[y_train == '1']\n",
    "sincere = X_train [y_train == '0']\n",
    "a = []\n",
    "y = []\n",
    "for sent in insincere:\n",
    "    a.append(glove_model(sent).vector)\n",
    "    y.append(1)\n",
    "for sent in sincere[:64674*4]:\n",
    "    a.append(glove_model(sent).vector)\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegressionCV(cv=5,random_state=0, solver='lbfgs').fit(a, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Test the model on 5% of the data}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "for sent in X_test:\n",
    "    x_test.append(glove_model(sent).vector)\n",
    "preds = LR_model.predict(x_test)\n",
    "preds = [int(x) for x in preds]\n",
    "y_test = [int(x) for x in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for Glove sentence embeddings: 0.5488594481522063\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score on LR for Glove sentence embeddings: \" + str(f1_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparisons:\n",
    "\n",
    "All the models were trained using Logistic Regression - Cross Validation method. Since we only trained the FastText and Glove enbedded data on only 25% of the dataset and tested them on a separate 5% of it (due to time-limit constraints given by the high computational complexity of the algorithms), please compare the Glove and FastText separate from the Word-bagging methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Average across the sentence wrod2vec embeddings}$\n",
    "\n",
    "| Word embedding |  F1 score|\n",
    "|----------------|  --------|\n",
    "|Glove       |$$0.5488$$|\n",
    "|FastText    |$$0.5380$$|\n",
    "\n",
    "$\\textbf{Word bagging methods}$\n",
    "\n",
    "|Bagging method | F1 score |\n",
    "|---------------|----------|\n",
    "|Simple  bagging|$$0.5400$$|\n",
    "|Tf-idf bagging|$$0.5530$$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we only dealt with string classification problem by tweaking the strings in different ways until we got a similar vector dimension to train our ML models. Either through averaging the word2vec embeddings or through word-bagging on a fixed vocabulary size, the LR-CS models received fixed input dimensions. This does not, however, fully represent a crucial aspect of this problem: the original data is varying in size and the ordering of it (the order in which the words come into a sentence) matters.\n",
    "\n",
    "A key method to better solve NLP problems is given by deep learning architectures. Both CNNs and RNNs are a common practice in this field. Usually, using one of the embeddings presented (Word2Vec with Skip-Gram or CBOW methods, it's FastText extension, Glove etc.), we get these feature vectors of the words and train a model with all of these, separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{CNN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN architectures have been used a lot in the NLP tasks, even though it would seem that RNNs should be better suited for these problems. One pretty interesting thing that is currently being researched now is using CNN architectures for character-level convolutions and feeding those CNN features to an NLP layers.\n",
    "\n",
    "More commonly used in CNN (and in RNN architectures for that matter) NLP tasks is to use a pre-trained word-embedding that has a fixed size for each word in the dictionary. Say that we have:\n",
    " - a sequence $x$ of $n$ entries \n",
    " - an encoding that gives us a $d$ dimensional vector for every word in the vocabulary\n",
    "\n",
    "Then we can solve our problem by using a convolutional architecture that has:\n",
    " - $d$ width of the filter, with a window size of an arbitrary $w$ (notice how this represents a sliding $w$-gram)\n",
    " - a padding large enough that we always have the same input size (padding can be defined as adding $d$ dimensional vectors of zeros until we have the same sentence length on all the questions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/NLP_CNN.png\"  style=\"width: 1400px; height: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{RNN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUs and LSTMs have the primary beneficial property that they can receive truly variable length inputs (by comparison, CNNs needed 0-padding on the input, so that all the training data would end up the same length). A basic RNN cell will receive both an input and state from the previous time-step and calculate an output and a state for the next time-step.\n",
    "\n",
    "Again, we need to consider some form of embedding for the words in our sentences. Using the same intuition, we can have a word embedding that gives us a $d$ dimentional input for each word that we found moving through the sentence. Having this, the way GRUs and LSTMs work is very similar\n",
    "\n",
    "$\\underline{GRU}$\n",
    "\n",
    "$z = \\sigma(x_tU^z + h_{t-1}W^z$\n",
    "\n",
    "$r = \\sigma(x_tU^r + h_{t-1}W^r$\n",
    "\n",
    "$s_t = tanh(x_tU^s + h_{t-1}W^s$\n",
    "\n",
    "$h_t = (1-z) \\circ s_t + z \\circ h_{t-1}$\n",
    "\n",
    "$\\underline{LSTM}$\n",
    "\n",
    "$i_t = \\sigma(x_tU^i + h_{t-1}W^i+b_i)$\n",
    "\n",
    "$f_t = \\sigma(x_tU^f+h_{t-1}W^f+b_f)$\n",
    "\n",
    "$o_t = \\sigma(x_tU^o + h_{t-1}W^o+b_o$\n",
    "\n",
    "$q_t = tanh(x_tU^q+h_{t-1}W^q+b_q$\n",
    "\n",
    "$p_t = f_t*p_{t-1}+i_t*q_t$\n",
    "\n",
    "$h_t=o_t*tanh(p_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/LSTM3-chain.png\"  style=\"width: 600px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
