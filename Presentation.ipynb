{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering on strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",header=None,low_memory=False)\n",
    "data_test = pd.read_csv(\"test.csv\",header=None,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data[1][1:]\n",
    "labels = data[2][1:]\n",
    "sentences_test = []\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer().fit(sentences)\n",
    "bag_of_words = vec.transform(sentences)\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ''the'' appears 665950 times.\n",
      "Word ''what'' appears 471294 times.\n",
      "Word ''is'' appears 443182 times.\n",
      "Word ''to'' appears 408009 times.\n",
      "Word ''in'' appears 378153 times.\n",
      "Word ''of'' appears 333515 times.\n",
      "Word ''how'' appears 290405 times.\n",
      "Word ''and'' appears 257922 times.\n",
      "Word ''do'' appears 253252 times.\n",
      "Word ''are'' appears 243038 times.\n"
     ]
    }
   ],
   "source": [
    "for word, freq in words_freq[:10]:\n",
    "    print(\"Word \" + \"'\\033[1m'\"+str(word) +\"'\\033[0m'\" + \" appears \" + str(freq) + \" times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset are 15999712 of which 97500 are distinct.\n"
     ]
    }
   ],
   "source": [
    "frequencies = np.array(words_freq)\n",
    "frequencies = [int(x) for x in frequencies[:,1]]\n",
    "distinct_words = int(len(vec.vocabulary_.items())/2)\n",
    "print(\"Total number of words in the dataset are \" + \n",
    "      \"\\033[1m\"+str(np.sum(frequencies)) +\"\\033[0m\"+ \" of which \" + \n",
    "      \"\\033[1m\"+str(distinct_words)+\"\\033[0m\" + \" are distinct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'uniersity' appears one time.\n",
      "Word 'wheath' appears one time.\n",
      "Word 'subjested' appears one time.\n",
      "Word 'notafcation' appears one time.\n",
      "Word 'faulds' appears one time.\n",
      "Word 'abody' appears one time.\n",
      "Word '15260' appears one time.\n",
      "Word 'localbitcoins' appears one time.\n",
      "Word 'issil' appears one time.\n",
      "Word 'c720' appears one time.\n"
     ]
    }
   ],
   "source": [
    "for word, freq in words_freq[-97500:-97490]:\n",
    "    print(\"Word \" + \"\\033[1m'\"+str(word) +\"'\\033[0m\" + \" appears one time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1960s 1)  (as 1)  (did 1)  (how 1)  (in 1)  (nation 1)  (nationalists 1)  (province 1)  (quebec 1)  (see 1)  (the 1)  (their 1)  \n",
      "How did Quebec nationalists see their province as a nation in the 1960s?\n",
      "\n",
      "(are 1)  (babies 3)  (dark 1)  (light 1)  (more 1)  (or 1)  (parents 1)  (skin 2)  (sweeter 1)  (their 1)  (to 1)  (which 1)  \n",
      "Which babies are more sweeter to their parents? Dark skin babies or light skin babies?\n"
     ]
    }
   ],
   "source": [
    "sincere_question_count = (bag_of_words[0].todense())\n",
    "sincere_question_words = np.argwhere(sincere_question_count>=1)\n",
    "for word in sincere_question_words:\n",
    "    print(\"(\"+vec.get_feature_names()[word[1]] + \" \" \n",
    "          + str(sincere_question_count[0,word[1]])+\")\", end=\"  \")\n",
    "print(\"\\n\" + sentences[1]+\"\\n\")\n",
    "lbls = labels.values\n",
    "insincere_example = np.argwhere(lbls == '1')[1,0]\n",
    "insincere_question_count = (bag_of_words[insincere_example].todense())\n",
    "insincere_question_words = np.argwhere(insincere_question_count>=1)\n",
    "for word in insincere_question_words:\n",
    "    print(\"(\"+vec.get_feature_names()[word[1]] + \" \" \n",
    "          + str(insincere_question_count[0,word[1]]) + \")\", end=\"  \")\n",
    "print(\"\\n\" + sentences[insincere_example+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[2][1:]\n",
    "Y = Y.values\n",
    "vectorizer = CountVectorizer(min_df=200)\n",
    "sentences = data[1][1:]\n",
    "X = vectorizer.fit_transform(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\n",
    "LR_model = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "preds = LR_model.predict(X_test)\n",
    "preds = [int(x) for x in preds]\n",
    "y_test = [int(x) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for bags of words: 0.5180409368081718\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score on LR for bags of words: \" + str(f1_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency- inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Instead of using the frequency of a word (that is, the number of times it appears in a sentence) in a given document (in our case, a sentence), we are going to furtherly tune the parameters so that extremly frequent words throughout the whole set of documents - like the words \"the\", \"what\", \"is\" and so on, shown above to have very high frequencies - will be taken less into acount, as they do not tell much about the nature of a given question.\n",
    "### $$tfidf(t,d,D) = tf(t,d)\\cdot idf(t,D)$$\n",
    "The above function multiplies the frequency of a word $t$ in the current document (sentence in our case) by the inverse frequency of that word in all the documents. These two functions, $tf$ and $idf$ are defined as:\n",
    "### $$idf(t,D)=log\\frac{N}{|\\{d\\in D : t\\in d\\}|}$$ \n",
    "with\n",
    " - $N$: number of documents in the corpus.\n",
    " - $|\\{d \\in D : t \\in d\\}|$: number of documents in the corpus in which t appears.\n",
    " and for the term-frequency fucntion\n",
    "### $$tf(t,d) = 0.5 + 0.5 \\cdot \\frac{f_{t,d}}{max\\{f_{t',d}:t'\\in d\\}}$$\n",
    "Which is an augmented frequency defined as the raw frequency of a word in the given document, divided by the maximum frequency of any element in that document (although mostly useless in our case, since this adjustment is done to prevent bias towards longer documents, and all of our documents are roughly the same size, them being only one question each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(min_df = 200)\n",
    "X = tfidf_vec.fit_transform(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\n",
    "LR_model = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "preds = LR_model.predict(X_test)\n",
    "preds = [int(x) for x in preds]\n",
    "y_test = [int(x) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for bags of words: 0.5232933579335793\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score on LR for bags of words: \" + str(f1_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram and continuous bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the Word2Vec methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary\n",
    " - decide what vocabulary size (select words from the document based on their number of appearances)\n",
    " - for each word in the vocabulary, assign a unique index and with those indeces, create one_hot vectors for them $x=[0,0,...,1, ... 0]$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/window.png\"  style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip gram\n",
    " - Try predicting a context (words frequently found nearby), given a single word input.\n",
    " - Define a window size $n$ (for example, 5, as in the image below), and pick the words that are $\\frac{n}{2}$ close to the word picked in a sentence (left and right)\n",
    " - Train a fully connected neural network with one hidden layer, predicting words found close to it. (For sentence \"a b c d e\", pick 'c' as the input and \"a b d e\" as the output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/SKIP_GRAM.png\"  style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous bags of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - similar to skip-gram, but \"flip\" the training inputs and outputs, i.e. predict a target output given context (words found close to it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/CBOW.png\"  style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim implementation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the sentences (get rid of the punctuation, lower-case the strings and split them into words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "sentences = data[1][1:]\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "embedding_sentences = []\n",
    "for sentence in sentences:\n",
    "    a = sentence.translate(translator).lower().split()\n",
    "    embedding_sentences.append(a)\n",
    "    all_words.extend(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a CBOW model, save it and use it after reloading to predict the most similar words in the dictionary to some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        embedding_sentences,\n",
    "        size=300,\n",
    "        window=5,\n",
    "        min_count=80,\n",
    "        seed=1,\n",
    "        workers=10)\n",
    "model.train(embedding_sentences, total_examples=len(embedding_sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CBOW_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_gbow_model  = gensim.models.Word2Vec.load(\"CBOW_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.7577846050262451), ('girl', 0.7545576095581055), ('lady', 0.6998194456100464), ('guy', 0.6641252636909485), ('person', 0.6589857935905457)]\n",
      "[('kings', 0.5375739336013794), ('emperor', 0.5216578841209412), ('queen', 0.5126441121101379), ('prince', 0.4900304079055786), ('julius', 0.48601898550987244)]\n",
      "[('puppy', 0.7504028677940369), ('kitten', 0.6995419263839722), ('hamster', 0.6757329702377319), ('cat', 0.6668018102645874), ('pet', 0.63416588306427)]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_gbow_model.wv.similar_by_word(\"woman\",5))\n",
    "print(loaded_gbow_model.wv.similar_by_word(\"king\",5))\n",
    "print(loaded_gbow_model.wv.similar_by_word(\"dog\",5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Skip-gram model, save it and use it after reloading to predict the most similar words in the dictionary to some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        embedding_sentences,\n",
    "        size=300,\n",
    "        window=5,\n",
    "        seed=1,\n",
    "        sg=1,\n",
    "        min_count=80,\n",
    "        workers=10)\n",
    "model.train(embedding_sentences, total_examples=len(embedding_sentences), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"SG_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sg_model = gensim.models.Word2Vec.load(\"SG_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.7577846050262451), ('girl', 0.7545576095581055), ('lady', 0.6998194456100464), ('guy', 0.6641252636909485), ('person', 0.6589857935905457)]\n",
      "[('kings', 0.5375739336013794), ('emperor', 0.5216578841209412), ('queen', 0.5126441121101379), ('prince', 0.4900304079055786), ('julius', 0.48601898550987244)]\n",
      "[('puppy', 0.7504028677940369), ('kitten', 0.6995419263839722), ('hamster', 0.6757329702377319), ('cat', 0.6668018102645874), ('pet', 0.63416588306427)]\n"
     ]
    }
   ],
   "source": [
    "print(load_sg_model.wv.similar_by_word(\"woman\",5))\n",
    "print(load_sg_model.wv.similar_by_word(\"king\",5))\n",
    "print(load_sg_model.wv.similar_by_word(\"dog\",5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "Extension of the Word2Vec models. The idea behind FastText is to use n-grams instead of words. An n-gram is a group of letters taken from the actual word (e.g., the 3-gram for \"apple\" will be \"app\", \"ppl\", \"ple\"), and the actual final embedding for the word will be the summ of all it's n-grams.\n",
    "\n",
    "What is great about this method is that we can extract a context (meaning) vector even for words that do not exist at all in the dictionary we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted = FastText(\n",
    "    embedding_sentences, \n",
    "    size=300, \n",
    "    window=5, \n",
    "    min_count=80, \n",
    "    workers=10,\n",
    "    sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted.save(\"FT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ft_model = gensim.models.FastText.load(\"FT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('womans', 0.7417962551116943), ('man', 0.7245641946792603), ('women', 0.6692850589752197), ('girl', 0.6474263072013855), ('lady', 0.5739828944206238)]\n",
      "[('kings', 0.6914445161819458), ('kingdoms', 0.5868042707443237), ('caesar', 0.5731709003448486), ('queen', 0.5708640813827515), ('kingdom', 0.5651408433914185)]\n",
      "[('dogs', 0.7345197200775146), ('puppy', 0.6820985078811646), ('kitten', 0.6240315437316895), ('kittens', 0.6132924556732178), ('puppies', 0.6027705669403076)]\n"
     ]
    }
   ],
   "source": [
    "print(load_ft_model.wv.similar_by_word(\"woman\",5))\n",
    "print(load_ft_model.wv.similar_by_word(\"king\",5))\n",
    "print(load_ft_model.wv.similar_by_word(\"dog\",5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that \"Gastroenteritis\" is not present in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word is not in the document\n"
     ]
    }
   ],
   "source": [
    "word_fq_array = np.array(words_freq)\n",
    "unique_words = dict.fromkeys(word_fq_array[:,0],1)\n",
    "if \"Gastroenteritis\" not in unique_words:\n",
    "    print(\"The word is not in the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similar meaning words according to the model. Notice how the fast text model will still produce decent similar words for the word \"Gastroenteritis\", even if we didn't have that word in the training corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('arthritis', 0.6741937398910522), ('asthma', 0.5665794610977173), ('veterinary', 0.5183663368225098), ('herpes', 0.5168657302856445), ('infections', 0.5165085792541504), ('medicines', 0.5158151388168335), ('antibiotics', 0.5150102376937866), ('swelling', 0.5117443799972534), ('constipation', 0.5098469853401184), ('pneumonia', 0.5062140822410583)]\n"
     ]
    }
   ],
   "source": [
    "print(load_ft_model.wv.most_similar(\"Gastroenteritis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_tf_model():\n",
    "    i = 0\n",
    "    Y = data[2][1:]\n",
    "    Y = Y.values\n",
    "    X = embedding_sentences\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05)\n",
    "    X_train = np.array(X_train)\n",
    "    insincere = X_train[y_train == '1'][:]\n",
    "    sincere = X_train [y_train == '0'][:]\n",
    "    a = []\n",
    "    y = []\n",
    "    for sent in insincere:\n",
    "        word_vectors = [] \n",
    "        for single_word in sent:\n",
    "            if single_word in load_ft_model.wv.vocab: \n",
    "                word_vectors.append(load_ft_model.wv.word_vec(single_word))\n",
    "        word_vectors = np.array(word_vectors)\n",
    "        if len(word_vectors) != 0:                \n",
    "            if word_vectors.shape[1] == 300:\n",
    "                a.append(np.mean(word_vectors,axis=0))\n",
    "                y.append(1)\n",
    "    for sent in sincere[:64674*4]:\n",
    "        word_vectors = []  \n",
    "        for single_word in sent:\n",
    "            if single_word in load_ft_model.wv.vocab: \n",
    "                word_vectors.append(load_ft_model.wv.word_vec(single_word))\n",
    "        word_vectors = np.array(word_vectors)\n",
    "        if len(word_vectors) != 0:\n",
    "            if word_vectors.shape[1] == 300:\n",
    "\n",
    "                a.append(np.mean(word_vectors,axis=0))\n",
    "                y.append(0)\n",
    "    a = np.array(a)\n",
    "    y = np.array(y)\n",
    "    LR_model = LogisticRegression(random_state=0, solver='lbfgs').fit(a, y)\n",
    "\n",
    "    x_test = []\n",
    "    indexes = []\n",
    "    for index, sent in enumerate(X_test):\n",
    "        word_vectors = []     \n",
    "        for single_word in sent:\n",
    "            if single_word in load_ft_model.wv.vocab: \n",
    "                word_vectors.append(load_ft_model.wv.word_vec(single_word))\n",
    "        word_vectors = np.array(word_vectors)\n",
    "        if len(word_vectors) != 0:   \n",
    "            if word_vectors.shape[1] == 300:\n",
    "                x_test.append(np.mean(word_vectors,axis=0))\n",
    "            else:\n",
    "                indexes.append(index)\n",
    "        else:\n",
    "            indexes.append(indexes)\n",
    "    \n",
    "    preds = LR_model.predict(x_test)\n",
    "    preds = [int(x) for x in preds]\n",
    "    y_test = [int(x) for x in y_test]\n",
    "    y_test = np.array(y_test)\n",
    "    if len(indexes) != 0:\n",
    "        y_test = np.delete(y_test,indexes)\n",
    "    print(\"F1 Score on LR for Fast text sentence embeddings: \" + str(f1_score(y_test,preds)))\n",
    "    return LR_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for Fast text sentence embeddings: 0.5490042951971886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lr_tf_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Vectors for Word Representation (Glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - matrix $X$ is the matrix of word co-occurances, in which $X_{ij}$ represents the number of times word $i$ occurs in the context of word $j$. \n",
    " - $X_{i} = \\sum_{k} X_{ik}$ be the total number of occurances in any context k for work i.\n",
    " - Finally, $P_{ij}=P(j|i)=X_{ij}/X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Probability and Ratio |$k = solid$|$k = gas$|$k = water$|$k = fashion$|\n",
    "|-----------------------|-----------|---------|-----------|-------------|\n",
    "|  $P(k|ice)$           |$1.9\\times 10^{-4}$|$6.6 \\times 10^{-5}$|$3.0\\times 10^{-3}$|$1.7\\times 10^{-5}$|\n",
    "|  $P(k|steam)$         |$2.2\\times 10^{-5}$|$7.8\\times 10^{-4}$|$2.2\\times 10^{-3}$|$1.8\\times 10^{-5}$|\n",
    "|  $P(k|ice)/P(k|steam)$|$$8.9$$            |$$8.5\\times 10^{-2}$$|$$1.36           $$|$$0.96$$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the probability ratio for various words $k$ illustrates pretty well the semantic similarity of two words $i$ and $j$.\n",
    " - k related to ice and not to steam: high ratio\n",
    " - k related to steam but not to ice: small ratio\n",
    " - k related to both: ratio close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization function proposed by Glove tries to minimize the below J function. Note that $V$ here represents the number of words in the whole vocabulary and the $f$ function will weight down the very frequent words (similar to the problem of link words described in the Word2Vec approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$J=\\sum_{i,j=1}^{V} f(X_{ij})(w_i^T\\tilde{w_j}+b_i+\\tilde{b_j}-logX_{ij})^2$$\n",
    "\n",
    "### $$f(X_{ij})= \\begin{cases} \n",
    "      (x/x_{max})^\\alpha & if x < x_{max} \\\\\n",
    "      1 & otherwise \\\\\n",
    "   \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model =  spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.EntityRecognizer at 0x166d97c9db0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sentences.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model(X[2]).vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[2][1:]\n",
    "Y = Y.values\n",
    "sentences = data[1][1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05)\n",
    "insincere = X_train[y_train == '1']\n",
    "sincere = X_train [y_train == '0']\n",
    "a = []\n",
    "y = []\n",
    "for sent in insincere:\n",
    "    a.append(glove_model(sent).vector)\n",
    "    y.append(1)\n",
    "for sent in sincere[:64674*4]:\n",
    "    a.append(glove_model(sent).vector)\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression(random_state=0, solver='lbfgs').fit(a, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for bags of words: 0.5617253830389383\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "for sent in X_test:\n",
    "    x_test.append(glove_model(sent).vector)\n",
    "preds = LR_model.predict(x_test)\n",
    "preds = [int(x) for x in preds]\n",
    "y_test = [int(x) for x in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on LR for Glove sentence embeddings: 0.5617253830389383\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score on LR for Glove sentence embeddings: \" + str(f1_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ELMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE:\n",
    "http://www.foldl.me/2014/glove-python/\n",
    "\n",
    "https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "### asd: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
